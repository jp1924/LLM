export WANDB_PROJECT="TNT"
export WANDB_RUN_GROUP='s2p-tnt'
export WANDB_WATCH=""

export TORCH_DISTRIBUTED_DEBUG="OFF"
export TORCHDYNAMO_DISABLE="1"
export PYTORCH_CUDA_ALLOC_CONF="expandable_segments:True,roundup_power2_divisions:[32:256,64:128,256:64,>:32]"
export CUDA_DEVICE_ORDER='PCI_BUS_ID'
export TOKENIZERS_PARALLELISM='false'
export OMP_NUM_THREADS=2
export TORCHINDUCTOR_COMPILE_THREAD=2

deepspeed --include=localhost:0,1,2,3 --master_port=9816 \
    /root/workspace/src/train.py \
    --output_dir='/root/output_dir/Qwen2.5-0.5B/s2p-tnt' \
    --cache_dir='/root/.cache/.[Qwen2.5-0.5B]preprocess/TNT_inst-s2p' \
    --run_name='Qwen/Qwen2.5-0.5B' \
    --model_name_or_path='Qwen/Qwen2.5-0.5B' \
    --preprocessing_batched=true \
    --preprocessing_num_workers=4 \
    --preprocessing_batch_size=1000 \
    --data_preprocessor_type='s2p_tnt' \
    --dataset_repo_ls \
        jp1924/TxtNumTxt \
    --data_name_map='{"jp1924/TxtNumTxt": "TNT"}' \
    --train_dataset_prefix='train' \
    --valid_dataset_prefix='validation' \
    --per_device_train_batch_size=64 \
    --gradient_accumulation_steps=1 \
    --per_device_eval_batch_size=12 \
    --eval_accumulation_steps=1 \
    --num_train_epochs=3 \
    --seed=42 \
    --do_train=true \
    --do_eval=true \
    --do_predict=false \
    --learning_rate=3e-5 \
    --lr_scheduler_type="cosine" \
    --warmup_ratio=0.1 \
    --weight_decay=0 \
    --eval_strategy='no' \
    --eval_steps=180 \
    --save_strategy='epoch' \
    --save_steps=180 \
    --logging_strategy='steps' \
    --logging_steps=1 \
    --data_max_length=512 \
    --bf16=true \
    --tf32=true \
    --report_to='wandb' \
    --ddp_timeout=18000000 \
    --do_data_main_process_first=true \
    --use_liger_kernel=false \
    --torch_compile=true \
    --dataloader_prefetch_factor=5 \
    --dataloader_num_workers=4 \
    --attn_implementation='flash_attention_2' \
    --remove_unused_columns=true \
    --packing=true \
    --packing_max_elem=20 \
    --gradient_checkpointing=true \
    --gradient_checkpointing_kwargs='{"use_reentrant": false}' \
    --include_num_input_tokens_seen=true \
    --include_tokens_per_second=true \
    --chat_template="{% for message in messages %}{{ '<|im_start|>' }}{% if message.role == 'spelling' %}{{ message.content }}{% elif message.role == 'phonetic' %}{{ message.content }}{% elif message.role == 'dual_script' and not add_generation_prompt %}{{ message.content }}{% endif %}{{ '<|im_end|>' }}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>' }}{% else %}{{ '<|endoftext|>' }}{% endif %}" \
    --predict_with_generate=true \
    --generation_config="/root/workspace/config/generation_config.json" \
    --deepspeed="/root/workspace/config/ZeRO_2_act_check.json" \
    --remove_unused_columns=false \
    --tokenizer_kwargs='{"pad_token": "<|box_end|>"}'